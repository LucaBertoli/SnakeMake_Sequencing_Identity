# Identity Calculation Workflow
## General description

## Repository content
This repository contains the code used to empirically evaluate sequencing quality and accuracy. It comprehends:
- SnakeMake_Sequencing_Identity: snakemake pipeline used to automatically produce the final report files
- scripts: collection of bash and python script used for the trimming, alignment, variant calling and graphical visualization.
- plots: legacy plots.

The snakeake workflow comprehends the following folders:

- Config
    - Run_Identity_Workflow.yml configuration yaml file, in which the key paths (input BAM, filtering VCF, filtering BED, etc.) and parameters (thread usage for multithread processing) should be specified.
- snakefile
    - Run_Identity_Workflow.smk file containing the pipeline main (rule all).
- rules
    - folder containing 17 snakefile, each containing independent processing rules executed by the main snakefile.
- scripts
    - folder containing 14 python scripts, executed by the workflow's rules.

The scripts folder contains the following graphical visualization Jupyter notebooks:
- plot_identity_BQ_stratifiction.ipynb
    - python script plotting the sequencing quality-stratified results 
- plot_identity_BQ_stratifiction_per_cycle.ipynb
    - python script plotting the sequencing-cycle stratified results
The paths reported in these jupyter notebooks need to be modified accordingly.
## Download
The current repository should be locally cloned:

    git clone https://github.com/Lab-Delledonne-bioinfo/calcolo_identita_sequenze_Sequenziatori


## Dependencies
The workflow has the following requirements:
- snakemake
- python3
- bedtools
- samtools
  
Additionally, the following python libraries are required:
- pysam
- sys
- gzip
- math
- collections
- re
- multiprocessing
- os
- argparse
- numpy
- csv
- pandas
- matplotlib
- seaborn

## Usage
The user should create an output directory and copy inside it the configuration and snakefile:

    mkdir results
    cd results
    cp <repo path>/SnakeMake_Sequencing_Identity/config/Run_Identity_Workflow.yml config.yaml
    cp <repo path>/SnakeMake_Sequencing_Identity/snakefile/Run_Identity_Workflow.smk snakefile

Then the user should modify the config.yaml according to its experimental settings, afterward the pipeline is run using:

    snakemake --cores 25

Use a multithreading option (--cores) coherent with the config.yaml file and appropriate for the infrostructure used.


## Further information
Run_Identity_Workflow.py
    This script takes the following inputs:

BAM

The BAM file must be generated by aligning raw reads with BWA-MEM without trimming, deduplication, or base quality recalibration.

VCF

The VCF must contain SNV/INDEL variants. You can use a reference VCF, such as the GIAB GoldSeq VCF for NA12878 (or other GIAB/Platinum Genome lines), or a custom VCF called for your specific sample (e.g., from GATK).

In the latter case, the VCF must be normalized using:
            bcftools norm $VCF -f $FASTA -m -both

output_name

The output name will be used as the base name for all files generated by the workflow.

The Run_Identity_Workflow.py script executes the following steps:
Workflow steps

    Intersection with high-confidence regions
    Using bcftools intersect, the BAM file is intersected with a BED file of high-confidence regions (e.g., HG001_GRCh38_1_22_v4.2.1_benchmark.bed) to restrict identity analysis to high-confidence regions (HCR).

    IdentityRevelations.py
    This script computes a .tsv.gz file containing mismatches, indels, and per-read identity values (excluding secondary, supplementary, and unmapped reads).
    Details are provided below.

    IdentityRevelations_stats.py
    This step generates statistics from the previous .tsv.gz file, including:

        Total number of reads

        Mean identity (average of per-read identity values)

        Counts of reads with 0, 1, 2, 3, or >3 mismatches
        The results are written to a .stats file.

    Scatter_BAM.py
    This script computes the mean Phred quality score of aligned bases for each read in a BAM file (excluding soft-clipped regions).
    The output is a _mean_quality.tsv.gz file.
    Optionally, it can split the BAM into two separate files based on mean quality.

    Identity_plot.py
    This script takes as input the .tsv.gz file with identity values (from IdentityRevelations.py) and the .tsv.gz file with Phred scores (from Scatter_BAM.py).
    It converts identity values to Q-scores using the formula:
    Q-score = -10 * log10(1 - identity)
    Since identity = 1 cannot be log-transformed, it is approximated as 0.999.
    The script then produces a plot comparing sequencer-assigned Q-scores with alignment-derived Q-scores.

    identity_calculalculation_all_types.sh
    This script takes as input the BAM, VCF, FASTA, target regions, and output_name to generate an additional report.
    Using samtools mpileup, it calculates identity across aligned data by counting total aligned non-variant bases (not overlapping positions in the VCF) and the number of mismatched/indel bases.
    The output is saved as .identity.all.types.pileup.report.

Example command to run the workflow

bash
bash Run_Identity_Workflow.sh $PATH/start_sorted.bam \
    $PATH/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \
    run_workflow_SAMPLE_NAME >> IDENTITY_run_workflow_SAMPLE_NAME.log

ðŸ§¬ IdentityRevelations.py

This script is the core component for calculating read identity based on alignment data.
It computes identity for each aligned read in a BAM file (excluding unmapped, secondary, and supplementary reads) using a VCF file containing SNV and INDEL variants.

For each read, it calculates four identity metrics â€” all expressed as values between 0 and 1, where 1 indicates perfect identity with the reference:

    identity
    Identity based only on the number of total mismatches (from the MD tag):
    identity=1âˆ’mismatchestotalaligned_lengthtotal
    identity=1âˆ’aligned_lengthtotalmismatchestotal

    identity_filtered
    Same as above, but excluding mismatches coinciding with known SNVs in the VCF:
    identityfiltered=1âˆ’mismatchesfilteredaligned_lengthtotal
    identityfiltered=1âˆ’aligned_lengthtotalmismatchesfiltered

    identity_with_indels
    Considers both mismatches and the total length of insertions and deletions:
    identitywith_indels=1âˆ’mismatchestotal+insertedbases+deletedbasesaligned_lengthtotal
    identitywith_indels=1âˆ’aligned_lengthtotalmismatchestotal+insertedbases+deletedbases

    identity_filtered_with_indels
    Same as above, but excluding both mismatches and indels overlapping known variants (SNV and INDEL):
    identityfiltered_with_indels=1âˆ’mismatchesfiltered+filteredinserted_bases+filtereddeleted_basesaligned_lengthtotal
    identityfiltered_with_indels=1âˆ’aligned_lengthtotalmismatchesfiltered+filteredinserted_bases+filtereddeleted_bases

Notes:

    aligned_length_total represents the total alignment length of the read.

    The identity_filtered_with_indels metric is the one used in subsequent pipeline steps.

Using the pysam library, the script extracts mismatches from the MD tag and indels from the CIGAR string.
These are compared against two dictionaries containing SNVs and INDELs from the input VCF.
Any indels or mismatches matching known variants are ignored in the identity calculation.

The output is a .tsv.gz file containing for each read:

    Total and filtered mismatch counts

    Counts and lengths of insertions/deletions (filtered and unfiltered)

    Aligned read length

    Identity values under all conditions described above
